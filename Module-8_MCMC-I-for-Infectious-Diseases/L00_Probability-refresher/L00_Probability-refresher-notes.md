---
title: "Probability Refresher Notes"
subtitle: "M08: Lecture 0 (Pages 1-8)"
author: "Callum Arnold"
---

# Random variables

- Random variable $X$ is a function or variable whose value is generated by a
  random experiment e.g. coin flip.

- E.g. Binomial random variable
  - $k$ successes in a fixed number of trials ($n$)
  - $X_i \sim \text{Bernoulli}(p)$
    - $X \in \{0, 1\} \text{ with } \Pr(X=1)=p, \Pr(X=0)=1-p \text{ for } 0 \le p \le 1$
  - The number of sucesses $S_n = \sum_{i=1}^n X_i$ is a *binomial r.v.*
  - $\Pr (S_n = k) = \binom{n}{k}p^k (1-p)^{n-k}$
- Negative binomial distribution
  - $X_1, X_2, ..., $ ordered $\text{Bernoulli}(p$
  - Let $N=\min\{n:X_n = k\}$ be the number of trials until $k$ successes
  - The first $n-1$ trials must result in $k-1$ successes, which can be modelled
    using a Binomial formula
    - $\binom{n-1}{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)}$
  - $n$th trial must be a success with probability $p$
$$
\begin{aligned}
    \Pr(N=n) &= p \times \binom{n-1}{k-1}p^{k-1}(1-p)^{(n-1)-(k-1)} \\
    &= \binom{n-1}{k-1}p^{k}(1-p)^{n-k} \\
    \text{for } n &= k, k+1, ..., \infin \\
    \mu &= \frac{k}{p} \\
    \sigma^2 &= \frac{k(1-p)}{p^2}
\end{aligned}
$$

- Geometric distribution is a special type of Negative Binomial distribution
  - Number of trials until the first success occurs i.e. $k=1$ for successes, or
    $n-r=1$ for failures
$$
\begin{aligned}
    \Pr(N=n) &= \binom{n-1}{k-1}p^{k-1} (1-p)^{n-1} \\
    &= p(1-p)^{n-1}
\end{aligned}
$$

- A function that assigns probabilities to r.v values is called a **probability
  mass function**
  - Can be more generally defined using a **cumulative distribution function**

## CDFs and PDFs

$$
\begin{aligned}
    F(x) &= \Pr(X \le x) \\
    f(x) &= \frac{d}{dx}F(x) \\
    \int_a^b f(x) dx &= F(b) - F(a) = \Pr(a \le X \le b) \text{ for } a \le b
\end{aligned}
$$

### Expectation

$$
\begin{aligned}
    E[g(X)] &= \int_{-\infin}^\infin g(x)dF(x) \\
    &= \sum_{k=1}^\infin g(x_k)\Pr(X=x_k) \text{ for discrete r.v. } X \\
    &= \int_{-\infin}^\infin g(x)f(x)dx \text{ for absolutely continuous r.v. } X \\
\end{aligned}
$$

### Example with exponential r.v

- Exponential r.v. has density $f(x) = \lambda e^{-\lambda x} 1_{\{x \ge 0\}}$
  - $\lambda > 0$ is the rate parameter
- Let $X \sim \text{Exp}(\lambda)$

$$
\begin{aligned}
    E(X) &= \int_0^\infin x\lambda e^{-\lambda x} dx \\ 
    &= \left[ 
      \begin{aligned}
        f(x) &= x \quad & g^\prime(x) &= e^{-\lambda x} \\
        f^\prime(x) &= 1 \quad & g(x) &= \frac{-e^{-\lambda x}}{\lambda}
      \end{aligned}
    \right] \\
    &= \lambda \left [
    \cancel{xe^{-\lambda x}} + \frac{1}{\lambda} \int e^{-\lambda x}
    \right]_0^\infin \\
    &= \left[
      \frac{-e^{-\lambda x}}{\lambda}
    \right]_0^\infin \\
    &= \frac{1}{\lambda}

\end{aligned}
$$

- Expectations are linear operations, so:
  - $E\left( \sum_{i=1}^n a_i X_i \right) = \sum_{i=1}^n a_i E(X_i)$
  - If the random variables $X_1, ..., X_n$ are independent:
    - $\text{Var}\left( \sum_{i=1}^n a_i X_i \right) = \sum_{i=1}^n a_i^2 \text{Var}(X_i)$

### Conditional probability

$$
\begin{aligned}
  \Pr(B|A) &= \frac{\Pr (A \cap B)}{\Pr(A)} \\
  E(X|A) &= \frac{E(X1_{\{A\}})}{\Pr(A)} \\ \\
  &\text{Discrete case} \\
  \Pr(X = x|Y=y) &= \frac{\Pr (X=x, Y=y)}{\Pr(Y=y)} \\\\
  &\text{Continuous case} \\
    F_{X|Y}(x|y) &= \frac{\int_{-\infin}^x f_{XY}(z, y)dz}{f_Y (y)} \\ 
  f_{X|Y}(x|y) &= \frac{f_{XY}(x, y)}{f_Y(y)} \quad \frac{\text{(Joint
  density)}}{\text{(Marginal density)}} \\
  \text{where: } f_Y(y) &= \int_{-\infin}^\infin f_{XY}(x, y) dx
\end{aligned}
$$

### Law of total probability

If $B_1, ..., B_n$ are mutually exclusive events

$$
\begin{aligned}
  \Pr(A) &= \sum_{i=1}^n \Pr \left( A \cap B_i \right) \\
  &= \sum_{i=1}^n \Pr (A | B_i)\Pr(B_i)
\end{aligned}
$$

### Law of total expectation

$E(X)$ is scalar, but $E(X|Y)$ is a r.v.

For discrete r.v.s $X, Y$:

$$
\begin{aligned}
  E(X|Y=y) &= \sum_{k=1}^\infin x_k \Pr (X=x | Y=y) \\
  \text{where: } E \left[ E(X|Y) \right] &= E[X]
\end{aligned}
$$

### Law of total variance

$$
\begin{aligned}
  \text{Var}(X) &= E(X^2) - E(X)^2 \\
  &= E \left[ E(X^2|Y) \right] - E \left[ E(X|Y) \right]^2 \\
  &= E \left[ \text{Var}(X|Y) + E(X|Y)^2 \right] -  E \left[ E(X|Y) \right]^2 \\
  &= E \left[ \text{Var}(X|Y)\right] + \left\{ E \left[E(X|Y)^2 \right] - E
  \left[E(X|Y) \right]^2 \right\} \\
  &= E \left[\text{Var}(X|Y)\right] + \text{Var}\left[E(X|Y)\right]
\end{aligned}
$$

### Strong Law of Large Numbers

The empirical average of iid r.vs converge to the theoretical expectation
(average).

$$
\underset{n \to \infin}{\lim} \frac{1}{n} \sum_{i=1}^n X_i = \mu
$$

### Central Limit Theorem

For large $n$, the empirical average behaves as $\mathcal{N}(\mu, \sigma^2 /n)$,
or, $\frac{\sqrt{n}(\bar{X_n} - \mu)}{\sigma} \sim \mathcal{N}(0, 1)$